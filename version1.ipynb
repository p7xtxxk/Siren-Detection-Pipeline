{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcMNjzGub1B_",
        "outputId": "14cb6dcc-d279-4dbc-b6c6-5915dcb82c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Path to your Minor Project folder\n",
        "PROJECT_DIR = Path('/content/drive/My Drive/Minor Project')\n",
        "\n",
        "# Create subfolders for siren and non-siren audio\n",
        "SIREN_DIR = PROJECT_DIR/'data'/'siren'\n",
        "NON_DIR   = PROJECT_DIR/'data'/'non_siren'\n",
        "\n",
        "for d in [SIREN_DIR, NON_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Project folder ready at:\", PROJECT_DIR)\n",
        "print(\"Siren folder:\", SIREN_DIR)\n",
        "print(\"Non-siren folder:\", NON_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxpjK7cRgjnh",
        "outputId": "f24fe36d-ef2b-4faa-9e69-ca30ea16a20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project folder ready at: /content/drive/My Drive/Minor Project\n",
            "Siren folder: /content/drive/My Drive/Minor Project/data/siren\n",
            "Non-siren folder: /content/drive/My Drive/Minor Project/data/non_siren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "PROJECT_DIR = Path('/content/drive/My Drive/Minor Project')\n",
        "\n",
        "# Subfolders for datasets\n",
        "ESC50_DIR = PROJECT_DIR/'datasets'/'ESC-50'\n",
        "URBAN_DIR = PROJECT_DIR/'datasets'/'UrbanSound8K'\n",
        "AUDIOSET_DIR = PROJECT_DIR/'datasets'/'AudioSet'\n",
        "\n",
        "for d in [ESC50_DIR, URBAN_DIR, AUDIOSET_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Dataset folders ready:\")\n",
        "print(\" - ESC-50:\", ESC50_DIR)\n",
        "print(\" - UrbanSound8K:\", URBAN_DIR)\n",
        "print(\" - AudioSet:\", AUDIOSET_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4atL80OhCrO",
        "outputId": "8a197390-6b2f-475b-fa54-d3cd79d3fbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset folders ready:\n",
            " - ESC-50: /content/drive/My Drive/Minor Project/datasets/ESC-50\n",
            " - UrbanSound8K: /content/drive/My Drive/Minor Project/datasets/UrbanSound8K\n",
            " - AudioSet: /content/drive/My Drive/Minor Project/datasets/AudioSet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ESC-50 dataset (2000 clips, 50 classes including siren, car horn, etc.)\n",
        "!wget -O /content/drive/My\\ Drive/Minor\\ Project/datasets/ESC-50/ESC-50-master.zip https://github.com/karoldvl/ESC-50/archive/master.zip\n",
        "!unzip -q /content/drive/My\\ Drive/Minor\\ Project/datasets/ESC-50/ESC-50-master.zip -d /content/drive/My\\ Drive/Minor\\ Project/datasets/ESC-50/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MYaLu7whIQ7",
        "outputId": "8c4d715c-b2cb-45f4-cb22-92b1c3e250a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-29 13:51:36--  https://github.com/karoldvl/ESC-50/archive/master.zip\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/karolpiczak/ESC-50/archive/master.zip [following]\n",
            "--2025-12-29 13:51:36--  https://github.com/karolpiczak/ESC-50/archive/master.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/karolpiczak/ESC-50/zip/refs/heads/master [following]\n",
            "--2025-12-29 13:51:36--  https://codeload.github.com/karolpiczak/ESC-50/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 20.27.177.114\n",
            "Connecting to codeload.github.com (codeload.github.com)|20.27.177.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘/content/drive/My Drive/Minor Project/datasets/ESC-50/ESC-50-master.zip’\n",
            "\n",
            "/Minor Project/data     [ <=>                ] 165.97M  24.8MB/s               ^C\n",
            "[/content/drive/My Drive/Minor Project/datasets/ESC-50/ESC-50-master.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/drive/My Drive/Minor Project/datasets/ESC-50/ESC-50-master.zip or\n",
            "        /content/drive/My Drive/Minor Project/datasets/ESC-50/ESC-50-master.zip.zip, and cannot find /content/drive/My Drive/Minor Project/datasets/ESC-50/ESC-50-master.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Path to ESC-50 metadata\n",
        "meta_path = Path('/content/drive/My Drive/Minor Project/datasets/ESC-50/ESC-50-master/meta/esc50.csv')\n",
        "esc_audio_dir = Path('/content/drive/My Drive/Minor Project/datasets/ESC-50/ESC-50-master/audio')\n",
        "\n",
        "# Target folders\n",
        "SIREN_DIR = Path('/content/drive/My Drive/Minor Project/data/siren')\n",
        "NON_DIR   = Path('/content/drive/My Drive/Minor Project/data/non_siren')\n",
        "\n",
        "df = pd.read_csv(meta_path)\n",
        "print(\"Classes available:\", df['category'].unique())\n",
        "\n",
        "# Filter siren vs non-siren\n",
        "siren_df = df[df['category'] == 'siren']\n",
        "non_df   = df[df['category'] != 'siren']\n",
        "\n",
        "print(\"Siren clips:\", len(siren_df))\n",
        "print(\"Non-siren clips:\", len(non_df))\n",
        "\n",
        "# Copy files into our working folders\n",
        "for _, row in siren_df.iterrows():\n",
        "    src = esc_audio_dir/row['filename']\n",
        "    dst = SIREN_DIR/row['filename']\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "for _, row in non_df.iterrows():\n",
        "    src = esc_audio_dir/row['filename']\n",
        "    dst = NON_DIR/row['filename']\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "print(\"Copied siren files to:\", SIREN_DIR)\n",
        "print(\"Copied non-siren files to:\", NON_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "RIRJjV6hhN4O",
        "outputId": "b9dc8740-13ca-4a10-d673-9980aed93785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Minor Project/datasets/ESC-50/ESC-50-master/meta/esc50.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-677888009.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mNON_DIR\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Minor Project/data/non_siren'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classes available:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Minor Project/datasets/ESC-50/ESC-50-master/meta/esc50.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Siren files:\", len(list(SIREN_DIR.glob(\"*.wav\"))))\n",
        "print(\"Non-siren files:\", len(list(NON_DIR.glob(\"*.wav\"))))\n"
      ],
      "metadata": {
        "id": "r9PwCNVjiKnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "SR = 22050\n",
        "WIN_SEC = 1.0\n",
        "HOP_SEC = 0.5\n",
        "WIN = int(SR * WIN_SEC)\n",
        "HOP = int(SR * HOP_SEC)\n",
        "\n",
        "def extract_features(x, sr=SR):\n",
        "    zcr = librosa.feature.zero_crossing_rate(y=x, frame_length=1024, hop_length=256)[0]\n",
        "    mfcc = librosa.feature.mfcc(y=x, sr=sr, n_mfcc=13, n_fft=1024, hop_length=256)\n",
        "    feats = []\n",
        "    feats += list(mfcc.mean(axis=1)) + list(mfcc.std(axis=1))  # 26\n",
        "    feats += [float(zcr.mean()), float(zcr.std())]             # +2 = 28\n",
        "    return np.array(feats, dtype=np.float32)\n",
        "\n",
        "def file_to_features(path, label):\n",
        "    x, sr = librosa.load(path.as_posix(), sr=SR, mono=True)\n",
        "    if np.max(np.abs(x)) > 0:\n",
        "        x = x / np.max(np.abs(x))\n",
        "    feats, labels = [], []\n",
        "    if len(x) < WIN:\n",
        "        x = np.pad(x, (0, WIN - len(x)))\n",
        "    for start in range(0, max(len(x) - WIN + 1, 1), HOP):\n",
        "        seg = x[start:start+WIN]\n",
        "        feats.append(extract_features(seg))\n",
        "        labels.append(label)\n",
        "    return np.stack(feats), np.array(labels)\n",
        "\n",
        "# Build dataset\n",
        "X_list, y_list = [], []\n",
        "for p in SIREN_DIR.glob(\"*.wav\"):\n",
        "    fx, fy = file_to_features(p, 1)\n",
        "    X_list.append(fx); y_list.append(fy)\n",
        "for p in NON_DIR.glob(\"*.wav\"):\n",
        "    fx, fy = file_to_features(p, 0)\n",
        "    X_list.append(fx); y_list.append(fy)\n",
        "\n",
        "X = np.concatenate(X_list, axis=0)\n",
        "y = np.concatenate(y_list, axis=0).astype(int)\n",
        "\n",
        "print(\"Feature matrix:\", X.shape)\n",
        "print(\"Labels:\", y.shape)\n",
        "print(\"Class balance:\", {c:int((y==c).sum()) for c in [0,1]})\n"
      ],
      "metadata": {
        "id": "UheBUyWFibiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train/test before balancing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Before SMOTE:\")\n",
        "print(\"Train balance:\", {c:int((y_train==c).sum()) for c in [0,1]})\n",
        "\n",
        "# Apply SMOTE only on training set\n",
        "sm = SMOTE(random_state=42, sampling_strategy='auto')\n",
        "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"After SMOTE:\")\n",
        "print(\"Train balance:\", {c:int((y_train_bal==c).sum()) for c in [0,1]})\n"
      ],
      "metadata": {
        "id": "m02iUXjQj299"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler().fit(X_train_bal)\n",
        "X_train_bal_n = scaler.transform(X_train_bal)\n",
        "X_test_n = scaler.transform(X_test)\n",
        "\n",
        "# Define ELM class\n",
        "class ELM:\n",
        "    def __init__(self, input_dim, hidden_dim=100, activation='relu', seed=42, l2=1e-3):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.W = rng.normal(0, 1, size=(hidden_dim, input_dim)).astype(np.float32)\n",
        "        self.b = rng.normal(0, 1, size=(hidden_dim,)).astype(np.float32)\n",
        "        self.activation = activation\n",
        "        self.beta = None\n",
        "        self.l2 = l2\n",
        "\n",
        "    def _act(self, H):\n",
        "        if self.activation == 'relu':\n",
        "            return np.maximum(0.0, H)\n",
        "        return 1.0 / (1.0 + np.exp(-H))  # sigmoid\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        H = self._act(X @ self.W.T + self.b)\n",
        "        A = H.T @ H + self.l2*np.eye(H.shape[1], dtype=np.float32)\n",
        "        self.beta = np.linalg.solve(A, H.T @ y.astype(np.float32))\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        H = self._act(X @ self.W.T + self.b)\n",
        "        z = H @ self.beta\n",
        "        return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "    def predict(self, X, thr=0.5):\n",
        "        return (self.predict_proba(X) >= thr).astype(int)\n",
        "\n",
        "# Train ELM\n",
        "elm = ELM(input_dim=X_train_bal_n.shape[1], hidden_dim=1000, activation='relu')\n",
        "elm.fit(X_train_bal_n, y_train_bal)\n",
        "\n",
        "# Evaluate on test set\n",
        "for thr in [0.55, 0.6, 0.65, 0.7]:\n",
        "    y_pred = elm.predict(X_test_n, thr=thr)\n",
        "    print(f\"Threshold {thr}\")\n",
        "    print(classification_report(y_test, y_pred, digits=4))\n",
        "\n"
      ],
      "metadata": {
        "id": "oASnGDU0kBek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa, numpy as np\n",
        "\n",
        "SR = 22050\n",
        "WIN_SEC = 1.5\n",
        "HOP_SEC = 0.75\n",
        "WIN = int(SR * WIN_SEC)\n",
        "HOP = int(SR * HOP_SEC)\n",
        "\n",
        "def extract_logmel(x, sr=SR, n_mels=64):\n",
        "    S = librosa.feature.melspectrogram(y=x, sr=sr, n_fft=1024, hop_length=256, n_mels=n_mels)\n",
        "    log_S = librosa.power_to_db(S, ref=np.max)\n",
        "    return log_S.astype(np.float32)\n",
        "\n",
        "def file_to_logmel(path, label):\n",
        "    x, sr = librosa.load(path.as_posix(), sr=SR, mono=True)\n",
        "    if np.max(np.abs(x)) > 0: x = x / np.max(np.abs(x))\n",
        "    feats, labels = [], []\n",
        "    if len(x) < WIN: x = np.pad(x, (0, WIN - len(x)))\n",
        "    for start in range(0, max(len(x) - WIN + 1, 1), HOP):\n",
        "        seg = x[start:start+WIN]\n",
        "        logmel = extract_logmel(seg)\n",
        "        feats.append(logmel)\n",
        "        labels.append(label)\n",
        "    return np.stack(feats), np.array(labels)\n"
      ],
      "metadata": {
        "id": "ZR31eb48kBX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_list, y_list = [], []\n",
        "for p in SIREN_DIR.glob(\"*.wav\"):\n",
        "    fx, fy = file_to_logmel(p, 1)\n",
        "    X_list.append(fx); y_list.append(fy)\n",
        "for p in NON_DIR.glob(\"*.wav\"):\n",
        "    fx, fy = file_to_logmel(p, 0)\n",
        "    X_list.append(fx); y_list.append(fy)\n",
        "\n",
        "X = np.concatenate(X_list, axis=0)\n",
        "y = np.concatenate(y_list, axis=0)\n",
        "\n",
        "# Add channel dimension for CNN\n",
        "X = X[..., np.newaxis]  # shape: (N, n_mels, time, 1)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "id": "iP_cRmoDlKl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "input_shape = X.shape[1:]  # (n_mels, time, 1)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3,3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "iViYY6vDlP1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "id": "FzkIGvj1lRcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}