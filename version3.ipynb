{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fa492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Audio config\n",
    "SR = 22050\n",
    "WIN_SEC = 1.5\n",
    "HOP_SEC = 0.75\n",
    "WIN = int(SR * WIN_SEC)\n",
    "HOP = int(SR * HOP_SEC)\n",
    "N_MELS = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b2b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_logmel(x, sr=SR, n_mels=N_MELS):\n",
    "    S = librosa.feature.melspectrogram(y=x, sr=sr, n_fft=1024, hop_length=256, n_mels=n_mels)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    return log_S.astype(np.float32)\n",
    "\n",
    "def file_to_logmel(path, label):\n",
    "    x, sr = librosa.load(path, sr=SR, mono=True)\n",
    "    if np.max(np.abs(x)) > 0:\n",
    "        x = x / np.max(np.abs(x))\n",
    "    feats, labels = [], []\n",
    "    if len(x) < WIN:\n",
    "        x = np.pad(x, (0, WIN - len(x)))\n",
    "    for start in range(0, max(len(x) - WIN + 1, 1), HOP):\n",
    "        seg = x[start:start+WIN]\n",
    "        feats.append(extract_logmel(seg))\n",
    "        labels.append(label)\n",
    "    return np.stack(feats), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e75488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sireNNet loader\n",
    "def load_siren_dataset(base_dir):\n",
    "    X_list, y_list = [], []\n",
    "    for cls in [\"ambulance\",\"firetruck\",\"police\"]:\n",
    "        folder = os.path.join(base_dir, cls)\n",
    "        for f in os.listdir(folder):\n",
    "            if f.lower().endswith(\".wav\"):\n",
    "                fx, fy = file_to_logmel(os.path.join(folder,f), 1) # siren = 1\n",
    "                X_list.append(fx); y_list.append(fy)\n",
    "    # traffic as non-siren\n",
    "    folder = os.path.join(base_dir, \"traffic\")\n",
    "    for f in os.listdir(folder):\n",
    "        if f.lower().endswith(\".wav\"):\n",
    "            fx, fy = file_to_logmel(os.path.join(folder,f), 0)\n",
    "            X_list.append(fx); y_list.append(fy)\n",
    "    return np.concatenate(X_list), np.concatenate(y_list)\n",
    "\n",
    "# UrbanSound8K loader\n",
    "def load_urbansound(base_dir, metadata_csv):\n",
    "    df = pd.read_csv(metadata_csv)\n",
    "    X_list, y_list = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        path = os.path.join(base_dir, \"fold\"+str(row[\"fold\"]), row[\"slice_file_name\"])\n",
    "        label = 1 if row[\"class\"] == \"siren\" else 0\n",
    "        fx, fy = file_to_logmel(path, label)\n",
    "        X_list.append(fx); y_list.append(fy)\n",
    "    return np.concatenate(X_list), np.concatenate(y_list)\n",
    "\n",
    "# ESC-50 loader\n",
    "def load_esc(base_dir, metadata_csv):\n",
    "    df = pd.read_csv(metadata_csv)\n",
    "    X_list, y_list = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        path = os.path.join(base_dir, \"audio\", row[\"filename\"])\n",
    "        label = 1 if row[\"category\"] == \"siren\" else 0\n",
    "        fx, fy = file_to_logmel(path, label)\n",
    "        X_list.append(fx); y_list.append(fy)\n",
    "    return np.concatenate(X_list), np.concatenate(y_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4983db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final dataset prepared\n",
      "Features shape: (46242, 64, 130, 1)\n",
      "Labels shape: (46242,)\n",
      "Train set shape: (36993, 64, 130, 1) Test set shape: (9249, 64, 130, 1)\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import librosa\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # --- Config ---\n",
    "# SR = 22050\n",
    "# WIN_SEC = 1.5\n",
    "# HOP_SEC = 0.75\n",
    "# WIN = int(SR * WIN_SEC)\n",
    "# HOP = int(SR * HOP_SEC)\n",
    "# N_MELS = 64\n",
    "\n",
    "# # --- Feature extraction helpers ---\n",
    "# def extract_logmel(x, sr=SR, n_mels=N_MELS):\n",
    "#     S = librosa.feature.melspectrogram(y=x, sr=sr, n_fft=1024, hop_length=256, n_mels=n_mels)\n",
    "#     log_S = librosa.power_to_db(S, ref=np.max)\n",
    "#     return log_S.astype(np.float32)\n",
    "\n",
    "# def file_to_logmel(path, label):\n",
    "#     x, sr = librosa.load(path, sr=SR, mono=True)\n",
    "#     if np.max(np.abs(x)) > 0:\n",
    "#         x = x / np.max(np.abs(x))\n",
    "#     feats, labels = [], []\n",
    "#     if len(x) < WIN:\n",
    "#         x = np.pad(x, (0, WIN - len(x)))\n",
    "#     for start in range(0, max(len(x) - WIN + 1, 1), HOP):\n",
    "#         seg = x[start:start+WIN]\n",
    "#         feats.append(extract_logmel(seg))\n",
    "#         labels.append(label)\n",
    "#     return np.stack(feats), np.array(labels)\n",
    "\n",
    "# # --- Dataset loaders ---\n",
    "# def load_siren_dataset(base_dir):\n",
    "#     X_list, y_list = [], []\n",
    "#     for cls in [\"ambulance\",\"firetruck\",\"police\"]:\n",
    "#         folder = os.path.join(base_dir, cls)\n",
    "#         for f in os.listdir(folder):\n",
    "#             if f.lower().endswith(\".wav\"):\n",
    "#                 fx, fy = file_to_logmel(os.path.join(folder,f), 1) # siren = 1\n",
    "#                 X_list.append(fx); y_list.append(fy)\n",
    "#     # traffic as non-siren\n",
    "#     folder = os.path.join(base_dir, \"traffic\")\n",
    "#     for f in os.listdir(folder):\n",
    "#         if f.lower().endswith(\".wav\"):\n",
    "#             fx, fy = file_to_logmel(os.path.join(folder,f), 0)\n",
    "#             X_list.append(fx); y_list.append(fy)\n",
    "#     return np.concatenate(X_list), np.concatenate(y_list)\n",
    "\n",
    "# def load_urbansound(base_dir, metadata_csv):\n",
    "#     df = pd.read_csv(metadata_csv)\n",
    "#     X_list, y_list = [], []\n",
    "#     for _, row in df.iterrows():\n",
    "#         path = os.path.join(base_dir, \"audio\", \"fold\"+str(row[\"fold\"]), row[\"slice_file_name\"])\n",
    "#         label = 1 if row[\"class\"] == \"siren\" else 0\n",
    "#         try:\n",
    "#             fx, fy = file_to_logmel(path, label)\n",
    "#             X_list.append(fx); y_list.append(fy)\n",
    "#         except Exception as e:\n",
    "#             print(\"Skipping file:\", path, \"Error:\", e)\n",
    "#     return np.concatenate(X_list), np.concatenate(y_list)\n",
    "\n",
    "# def load_esc(base_dir, metadata_csv):\n",
    "#     df = pd.read_csv(metadata_csv)\n",
    "#     X_list, y_list = [], []\n",
    "#     for _, row in df.iterrows():\n",
    "#         path = os.path.join(base_dir, \"audio\", row[\"filename\"])\n",
    "#         label = 1 if row[\"category\"] == \"siren\" else 0\n",
    "#         try:\n",
    "#             fx, fy = file_to_logmel(path, label)\n",
    "#             X_list.append(fx); y_list.append(fy)\n",
    "#         except Exception as e:\n",
    "#             print(\"Skipping file:\", path, \"Error:\", e)\n",
    "#     return np.concatenate(X_list), np.concatenate(y_list)\n",
    "\n",
    "# --- Load datasets with absolute paths ---\n",
    "# --- Load datasets with absolute paths ---\n",
    "\n",
    "# UrbanSound8K (fix: include \"audio\" in path)\n",
    "# --- Load datasets with absolute paths ---\n",
    "\n",
    "# UrbanSound8K → loader will add \"audio/foldX/...\"\n",
    "X_us, y_us = load_urbansound(\n",
    "    r\"C:\\Users\\prate\\Downloads\\College Academics\\Minor Project\\Minor Project\\datasets\\UrbanSound8k\\UrbanSound8K\\audio\",\n",
    "    r\"C:\\Users\\prate\\Downloads\\College Academics\\Minor Project\\Minor Project\\datasets\\UrbanSound8k\\UrbanSound8K\\metadata\\UrbanSound8K.csv\"\n",
    ")\n",
    "\n",
    "# ESC-50 → loader will add \"audio/filename\"\n",
    "X_esc, y_esc = load_esc(\n",
    "    r\"C:\\Users\\prate\\Downloads\\College Academics\\Minor Project\\Minor Project\\datasets\\ESC-50\\ESC-50-master\",\n",
    "    r\"C:\\Users\\prate\\Downloads\\College Academics\\Minor Project\\Minor Project\\datasets\\ESC-50\\ESC-50-master\\meta\\esc50.csv\"\n",
    ")\n",
    "\n",
    "# sireNNet → direct folder structure already matches\n",
    "X_sirenNet, y_sirenNet = load_siren_dataset(\n",
    "    r\"C:\\Users\\prate\\Downloads\\College Academics\\Minor Project\\Minor Project\\datasets\\sireNNet\"\n",
    ")\n",
    "\n",
    "# --- Merge all datasets ---\n",
    "X_all = np.concatenate([X_us, X_esc, X_sirenNet], axis=0)[..., np.newaxis]\n",
    "y_all = np.concatenate([y_us, y_esc, y_sirenNet], axis=0)\n",
    "\n",
    "print(\"✅ Final dataset prepared\")\n",
    "print(\"Features shape:\", X_all.shape)\n",
    "print(\"Labels shape:\", y_all.shape)\n",
    "\n",
    "# --- Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, stratify=y_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train set shape:\", X_train.shape, \"Test set shape:\", X_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f10bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,433</span> (415.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,433\u001b[0m (415.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,953</span> (413.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m105,953\u001b[0m (413.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">480</span> (1.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m480\u001b[0m (1.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9039 - loss: 0.2410\n",
      "Epoch 1: val_loss improved from None to 0.15291, saving model to best_siren_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: finished saving model to best_siren_model.h5\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 131ms/step - accuracy: 0.9412 - loss: 0.1665 - val_accuracy: 0.9568 - val_loss: 0.1529\n",
      "Epoch 2/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9687 - loss: 0.0929\n",
      "Epoch 2: val_loss improved from 0.15291 to 0.07898, saving model to best_siren_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: finished saving model to best_siren_model.h5\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 129ms/step - accuracy: 0.9714 - loss: 0.0852 - val_accuracy: 0.9742 - val_loss: 0.0790\n",
      "Epoch 3/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9777 - loss: 0.0660\n",
      "Epoch 3: val_loss improved from 0.07898 to 0.06814, saving model to best_siren_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: finished saving model to best_siren_model.h5\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 131ms/step - accuracy: 0.9796 - loss: 0.0621 - val_accuracy: 0.9750 - val_loss: 0.0681\n",
      "Epoch 4/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9818 - loss: 0.0545\n",
      "Epoch 4: val_loss did not improve from 0.06814\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 125ms/step - accuracy: 0.9828 - loss: 0.0515 - val_accuracy: 0.9646 - val_loss: 0.1169\n",
      "Epoch 5/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9860 - loss: 0.0436\n",
      "Epoch 5: val_loss did not improve from 0.06814\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 125ms/step - accuracy: 0.9862 - loss: 0.0432 - val_accuracy: 0.9711 - val_loss: 0.0771\n",
      "Epoch 6/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9885 - loss: 0.0364\n",
      "Epoch 6: val_loss did not improve from 0.06814\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 125ms/step - accuracy: 0.9885 - loss: 0.0359 - val_accuracy: 0.9523 - val_loss: 0.1586\n",
      "Epoch 7/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9887 - loss: 0.0353\n",
      "Epoch 7: val_loss did not improve from 0.06814\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 124ms/step - accuracy: 0.9897 - loss: 0.0321 - val_accuracy: 0.9793 - val_loss: 0.0692\n",
      "Epoch 8/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9922 - loss: 0.0247\n",
      "Epoch 8: val_loss improved from 0.06814 to 0.03872, saving model to best_siren_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: finished saving model to best_siren_model.h5\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 126ms/step - accuracy: 0.9914 - loss: 0.0259 - val_accuracy: 0.9874 - val_loss: 0.0387\n",
      "Epoch 9/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9927 - loss: 0.0225\n",
      "Epoch 9: val_loss did not improve from 0.03872\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 125ms/step - accuracy: 0.9921 - loss: 0.0236 - val_accuracy: 0.9738 - val_loss: 0.1057\n",
      "Epoch 10/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9935 - loss: 0.0188\n",
      "Epoch 10: val_loss did not improve from 0.03872\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 123ms/step - accuracy: 0.9931 - loss: 0.0201 - val_accuracy: 0.9811 - val_loss: 0.0602\n",
      "Epoch 11/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9939 - loss: 0.0196\n",
      "Epoch 11: val_loss did not improve from 0.03872\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 125ms/step - accuracy: 0.9937 - loss: 0.0203 - val_accuracy: 0.9761 - val_loss: 0.0990\n",
      "Epoch 12/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9951 - loss: 0.0163\n",
      "Epoch 12: val_loss did not improve from 0.03872\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 124ms/step - accuracy: 0.9950 - loss: 0.0171 - val_accuracy: 0.9843 - val_loss: 0.0709\n",
      "Epoch 13/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9939 - loss: 0.0192\n",
      "Epoch 13: val_loss improved from 0.03872 to 0.03393, saving model to best_siren_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: finished saving model to best_siren_model.h5\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 132ms/step - accuracy: 0.9943 - loss: 0.0182 - val_accuracy: 0.9909 - val_loss: 0.0339\n",
      "Epoch 14/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9957 - loss: 0.0146\n",
      "Epoch 14: val_loss improved from 0.03393 to 0.02265, saving model to best_siren_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: finished saving model to best_siren_model.h5\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 129ms/step - accuracy: 0.9948 - loss: 0.0161 - val_accuracy: 0.9935 - val_loss: 0.0227\n",
      "Epoch 15/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9961 - loss: 0.0104\n",
      "Epoch 15: val_loss did not improve from 0.02265\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 131ms/step - accuracy: 0.9957 - loss: 0.0132 - val_accuracy: 0.9788 - val_loss: 0.0909\n",
      "Epoch 16/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9964 - loss: 0.0118\n",
      "Epoch 16: val_loss did not improve from 0.02265\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 125ms/step - accuracy: 0.9958 - loss: 0.0139 - val_accuracy: 0.9795 - val_loss: 0.0771\n",
      "Epoch 17/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9963 - loss: 0.0133\n",
      "Epoch 17: val_loss did not improve from 0.02265\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 125ms/step - accuracy: 0.9958 - loss: 0.0147 - val_accuracy: 0.9904 - val_loss: 0.0304\n",
      "Epoch 18/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9956 - loss: 0.0138\n",
      "Epoch 18: val_loss did not improve from 0.02265\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 127ms/step - accuracy: 0.9965 - loss: 0.0113 - val_accuracy: 0.9922 - val_loss: 0.0263\n",
      "Epoch 19/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9962 - loss: 0.0112\n",
      "Epoch 19: val_loss did not improve from 0.02265\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 125ms/step - accuracy: 0.9959 - loss: 0.0126 - val_accuracy: 0.9939 - val_loss: 0.0228\n",
      "Epoch 20/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9960 - loss: 0.0153\n",
      "Epoch 20: val_loss did not improve from 0.02265\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 125ms/step - accuracy: 0.9966 - loss: 0.0119 - val_accuracy: 0.9946 - val_loss: 0.0235\n",
      "Epoch 21/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9971 - loss: 0.0092\n",
      "Epoch 21: val_loss did not improve from 0.02265\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 125ms/step - accuracy: 0.9968 - loss: 0.0097 - val_accuracy: 0.9928 - val_loss: 0.0305\n",
      "Epoch 22/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9976 - loss: 0.0075\n",
      "Epoch 22: val_loss did not improve from 0.02265\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 126ms/step - accuracy: 0.9972 - loss: 0.0090 - val_accuracy: 0.9928 - val_loss: 0.0288\n",
      "Epoch 23/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9978 - loss: 0.0073\n",
      "Epoch 23: val_loss did not improve from 0.02265\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 126ms/step - accuracy: 0.9976 - loss: 0.0082 - val_accuracy: 0.9923 - val_loss: 0.0348\n",
      "Epoch 24/130\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9970 - loss: 0.0092\n",
      "Epoch 24: val_loss did not improve from 0.02265\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 125ms/step - accuracy: 0.9969 - loss: 0.0096 - val_accuracy: 0.9911 - val_loss: 0.0306\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9919 - loss: 0.0269\n",
      "✅ Unified Siren Model Test accuracy: 0.9918910264968872\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# --- Optimized CNN model ---\n",
    "model_siren = models.Sequential([\n",
    "    layers.Conv2D(16, (3,3), activation='relu', padding='same', input_shape=X_train.shape[1:]),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model_siren.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_siren.summary()\n",
    "\n",
    "# --- Callbacks ---\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,              # stop if no improvement for 10 epochs\n",
    "    restore_best_weights=True # roll back to best weights\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"best_siren_model.h5\",    # file to save the best model\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "history = model_siren.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=130,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")\n",
    "\n",
    "# --- Evaluate ---\n",
    "test_loss, test_acc = model_siren.evaluate(X_test, y_test)\n",
    "print(\"✅ Unified Siren Model Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13184eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Siren       0.99      1.00      1.00      7731\n",
      "       Siren       0.98      0.97      0.98      1518\n",
      "\n",
      "    accuracy                           0.99      9249\n",
      "   macro avg       0.99      0.99      0.99      9249\n",
      "weighted avg       0.99      0.99      0.99      9249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions\n",
    "y_pred = (model_siren.predict(X_test) > 0.215).astype(\"int32\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Siren\",\"Siren\"], yticklabels=[\"Non-Siren\",\"Siren\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Non-Siren\",\"Siren\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b3de58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "🎵 Random Siren Test\n",
      "True Label: 1 (1 = Siren)\n",
      "Predicted Probability: 1.0000\n",
      "Predicted Label: 1 (Threshold=0.215)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Pick a random index where the true label is Siren (1)\n",
    "siren_indices = np.where(y_test == 1)[0]\n",
    "rand_idx = random.choice(siren_indices)\n",
    "\n",
    "# Extract the sample\n",
    "x_sample = X_test[rand_idx:rand_idx+1]   # keep batch dimension\n",
    "y_true = y_test[rand_idx]\n",
    "\n",
    "# Predict probability\n",
    "y_prob = model_siren.predict(x_sample).ravel()[0]\n",
    "\n",
    "# Apply tuned threshold\n",
    "THRESH = 0.215\n",
    "y_pred = int(y_prob > THRESH)\n",
    "\n",
    "print(\"🎵 Random Siren Test\")\n",
    "print(f\"True Label: {y_true} (1 = Siren)\")\n",
    "print(f\"Predicted Probability: {y_prob:.4f}\")\n",
    "print(f\"Predicted Label: {y_pred} (Threshold={THRESH})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "350d1f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "🎵 Random Non-Siren Test\n",
      "True Label: 0 (0 = Non-Siren)\n",
      "Predicted Probability: 0.0000\n",
      "Predicted Label: 0 (Threshold=0.215)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Pick a random index where the true label is Non-Siren (0)\n",
    "nonsiren_indices = np.where(y_test == 0)[0]\n",
    "rand_idx = random.choice(nonsiren_indices)\n",
    "\n",
    "# Extract the sample\n",
    "x_sample = X_test[rand_idx:rand_idx+1]   # keep batch dimension\n",
    "y_true = y_test[rand_idx]\n",
    "\n",
    "# Predict probability\n",
    "y_prob = model_siren.predict(x_sample).ravel()[0]\n",
    "\n",
    "# Apply tuned threshold\n",
    "THRESH = 0.215\n",
    "y_pred = int(y_prob > THRESH)\n",
    "\n",
    "print(\"🎵 Random Non-Siren Test\")\n",
    "print(f\"True Label: {y_true} (0 = Non-Siren)\")\n",
    "print(f\"Predicted Probability: {y_prob:.4f}\")\n",
    "print(f\"Predicted Label: {y_pred} (Threshold={THRESH})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93275d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step\n",
      "🔍 Best threshold: 0.292\n",
      "Precision: 0.983, Recall: 0.972, F1: 0.978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_prob = model_siren.predict(X_test).ravel()\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_thresh = thresholds[best_idx]\n",
    "\n",
    "print(f\"🔍 Best threshold: {best_thresh:.3f}\")\n",
    "print(f\"Precision: {precision[best_idx]:.3f}, Recall: {recall[best_idx]:.3f}, F1: {f1_scores[best_idx]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8d2ccda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step\n",
      "Confusion Matrix:\n",
      " [[7700   31]\n",
      " [  39 1479]]\n",
      "TP: 1479, TN: 7700, FP: 31, FN: 39\n",
      "Calculated Accuracy: 0.9924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predictions with tuned threshold\n",
    "THRESH = 0.215\n",
    "y_prob = model_siren.predict(X_test).ravel()\n",
    "y_pred = (y_prob > THRESH).astype(\"int32\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# cm layout:\n",
    "# [[TN, FP],\n",
    "#  [FN, TP]]\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Accuracy formula\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "print(f\"Calculated Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a04fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: siren_savedmodel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: siren_savedmodel\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'siren_savedmodel'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 130, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2636689481104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689482064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689481680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689481488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689480912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689481872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689482256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689483216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689482832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689482640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689480720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689483024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689480336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689484176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689483792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689483600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689481296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689483984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689484752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689485712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689485136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689484944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689484368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689485328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689485520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689486864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689486480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689487440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\prate\\AppData\\Local\\Temp\\tmpijxktgvx\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\prate\\AppData\\Local\\Temp\\tmpijxktgvx\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\prate\\AppData\\Local\\Temp\\tmpijxktgvx'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 130, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2636689481104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689482064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689481680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689481488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689480912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689481872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689482256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689483216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689482832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689482640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689480720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689483024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689480336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689484176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689483792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689483600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689481296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689483984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689484752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689485712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689485136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689484944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689484368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689485328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689485520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689486864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689486480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2636689487440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "✅ Model exported in .h5, .keras, SavedModel, and .tflite formats\n",
      "✅ Model exported in all formats: .h5, SavedModel, .tflite\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model(\"best_siren_model.h5\")\n",
    "\n",
    "# 1. Save as HDF5 (.h5)\n",
    "model.save(\"siren_model.h5\")\n",
    "\n",
    "# 2. Save as native Keras format (.keras)\n",
    "model.save(\"siren_model.keras\")\n",
    "\n",
    "# 3. Export as TensorFlow SavedModel (directory format)\n",
    "model.export(\"siren_savedmodel\")\n",
    "\n",
    "# 4. Convert to TensorFlow Lite (.tflite)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open(\"siren_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"✅ Model exported in .h5, .keras, SavedModel, and .tflite formats\")\n",
    "\n",
    "\n",
    "# # 4. Convert to ONNX (.onnx)\n",
    "# spec = (tf.TensorSpec(model.inputs[0].shape, tf.float32, name=\"input\"),)\n",
    "# onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "# onnx.save(onnx_model, \"siren_model.onnx\")\n",
    "\n",
    "# print(\"✅ Model exported in all formats: .h5, SavedModel, .tflite, .onnx\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
